{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mi primer EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bloque de código para las bibliotecas que vamos a ir necesitando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import folium\n",
    "from branca.element import Element \n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "# modelado\n",
    "from sklearn.model_selection import train_test_split #división de datos del modelo ML\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# escalado\n",
    "from sklearn.preprocessing import StandardScaler # implementar escalado\n",
    "from sklearn.preprocessing import MinMaxScaler #implementar el escalado\n",
    "from pickle import dump #dump: Función para guardar objetos en un archivo, en este caso el escalador.\n",
    "\n",
    "# encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#entrenamiento\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Almacenamos la base de datos con la que vamos a trabajar y comprobamos que es correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Crear carpeta para almacenar datos en crudo\n",
    "os.makedirs('./data/raw', exist_ok=True)\n",
    "\n",
    "# Paso 2: Descargar y almacenar el archivo\n",
    "url = \"https://raw.githubusercontent.com/4GeeksAcademy/data-preprocessing-project-tutorial/main/AB_NYC_2019.csv\"\n",
    "file_path = './data/raw/AB_NYC_2019.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Guardar una copia local en la carpeta indicada\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Paso 3: Cargar el conjunto de datos\n",
    "# Verificamos las primeras filas para inspeccionar la estructura y composición del conjunto de datos\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conociendo el Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las dimensiones\n",
    "print(\"Filas , Columnas\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos información sobre los tipos de datos y valores no nulos para más tarde  \n",
    " poder clasificarlos, modificarlos o anularnos en caso de no ser necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener información sobre tipos de datos y valores no nulos\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trabajando los duplicados.\n",
    "\n",
    "Usamos el método duplicated() para detectar duplicados en un DataFrame y\n",
    "sum() para contar el número de duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicados = df.duplicated()\n",
    "num_duplicados = duplicados.sum()\n",
    "print(f\"En este caso en contramos {num_duplicados} duplicados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que haya duplicados los seleccionamos, eliminamos o modificamos a un valor que concuerde con la info del Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para seleccionar duplicados:\n",
    "\n",
    "#df_duplicados = df[duplicados]\n",
    "\n",
    "#método drop_duplicates() para eliminar filas duplicadas. Se puede indicar el conjunto de atributos a considerar.\n",
    "\n",
    "#df_sin_duplicados = df.drop_duplicates()\n",
    "\n",
    "# ejemplo control de duplicados:\n",
    "\n",
    "#df.duplicated().sum()\n",
    "# sin considerar el id\n",
    "#df.drop(\"Id\", axis = 1).duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Los Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver en last_reviews & en reviews_per_month hay bastantes nulos, es posible que prescindamos de estas columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].isnull().sum() != 0:\n",
    "        print(\"=======================================================\")\n",
    "        print(f\"{column} ==> Missing Values : {df[column].isnull().sum()}, dtypes : {df[column].dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rellenaremos los nulos de reviews_per_month con ceros para poder trabajar esta columna en caso de que sea necesaria y comprobamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.copy()\n",
    "df_new['reviews_per_month'] = df_new['reviews_per_month'].fillna(0)\n",
    "\n",
    "df_new.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Eliminando atributos no relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(['id', 'name', 'host_id', 'host_name', 'last_review'], axis=1)\n",
    "df_new.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a juntar en una sola columna la latitud y la longitud para tener las coordenadas en caso de que pueda ser util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar latitude y longitude en una columna 'coordinates' como tuple\n",
    "df_new['coordinates'] = df_new.apply(lambda row: (row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "# Eliminar las columnas originales si ya no son necesarias\n",
    "df_new.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "\n",
    "# Mostrar el resultado\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a chequear los nulos de nuevo, ya tendríamos limpios los datos en este ámbito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.isnull().sum().sort_values(ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Variables Categóricas y numéricas - Limpiando el Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos de nuevo las clases de variables para poder luego seleccionarlas, dividirlas y eliminarlas en caso de no ser necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar las variables numéricas\n",
    "variables_numericas = df_new._get_numeric_data().columns\n",
    "print(\"Las variables numéricas son:\")\n",
    "for var in variables_numericas:\n",
    "    print(f\" - {var}\")\n",
    "\n",
    "# Identificar las variables categóricas\n",
    "variables_categoricas = set(df_new.columns) - set(variables_numericas)\n",
    "print(\"\\nLas variables categóricas son:\")\n",
    "for var in variables_categoricas:\n",
    "    print(f\" - {var}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Análisis de las variables categóricas\n",
    "-Empezaremos con un conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.neighbourhood.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.room_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.neighbourhood_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.coordinates.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizando si necesitamos las coordenadas para trabajar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calcular el precio promedio total\n",
    "average_price_total = df_new['price'].mean()\n",
    "\n",
    "# Calcular el precio promedio por coordenada\n",
    "price_by_coordinates = df_new.groupby('coordinates')['price'].mean()\n",
    "\n",
    "# Crear un mapa centrado en Nueva York\n",
    "ny_map = folium.Map(location=[40.7128, -74.0060], zoom_start=10)\n",
    "\n",
    "# Función para asignar colores según el precio promedio\n",
    "def color_for_price(price):\n",
    "    if price < 100:\n",
    "        return 'green'\n",
    "    elif price < 300:\n",
    "        return 'orange'\n",
    "    else:\n",
    "        return 'red'\n",
    "\n",
    "# Agregar marcadores para las 500 coordenadas más frecuentes (según precio promedio)\n",
    "for coord, avg_price in price_by_coordinates.head(500).items():\n",
    "    folium.CircleMarker(\n",
    "        location=[coord[0], coord[1]],  # Coordenadas (lat, long)\n",
    "        radius=avg_price / 50,  # Ajustar el divisor para reducir el tamaño del marcador\n",
    "        color=color_for_price(avg_price),  # Color del borde según el precio\n",
    "        fill=True,\n",
    "        fill_color=color_for_price(avg_price),  # Color de relleno\n",
    "        fill_opacity=0.6,\n",
    "        tooltip=f\"<b>Coordenadas:</b> {coord}<br><b>Precio Promedio:</b> ${avg_price:.2f}\"\n",
    "    ).add_to(ny_map)\n",
    "\n",
    "# Crear una leyenda manual en HTML con el precio promedio global\n",
    "legend_html = f\"\"\"\n",
    "<div style=\"\n",
    "    position: fixed;\n",
    "    bottom: 50px;\n",
    "    left: 50px;\n",
    "    width: 250px;\n",
    "    height: 150px;\n",
    "    background-color: white;\n",
    "    border:2px solid grey;\n",
    "    z-index:9999;\n",
    "    font-size:14px;\n",
    "    padding: 10px;\n",
    "    border-radius: 10px;\">\n",
    "    <b>Rango de Precios</b><br>\n",
    "    <i style=\"background:green; width: 10px; height: 10px; float:left; margin-right: 10px; border-radius: 50%;\"></i> Menor a $100<br>\n",
    "    <i style=\"background:orange; width: 10px; height: 10px; float:left; margin-right: 10px; border-radius: 50%;\"></i> $100 a $300<br>\n",
    "    <i style=\"background:red; width: 10px; height: 10px; float:left; margin-right: 10px; border-radius: 50%;\"></i> Mayor a $300<br>\n",
    "    <br>\n",
    "    <b>Precio Promedio Global:</b> ${average_price_total:.2f}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Agregar la leyenda al mapa\n",
    "ny_map.get_root().html.add_child(Element(legend_html))\n",
    "\n",
    "# Mostrar el mapa interactivo\n",
    "ny_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de la Distribución de Precios:\n",
    "\n",
    "-Los círculos verdes indican áreas con precios promedio bajos (menores a $100).  \n",
    "-Los círculos naranjas representan zonas con precios promedio moderados ($100 a $300).  \n",
    "-Los círculos rojos muestran ubicaciones con precios promedio altos (mayores a $300).  \n",
    "  \n",
    "Los círculos rojos más grandes están concentrados en áreas urbanas densas, como Manhattan, lo que refleja una mayor demanda y precios más altos. Aún así podremos encontrar ofertas con precios precios más moderados en casi todas las zonas.  \n",
    "\n",
    "Teniendo en cuenta que ya contamos con neighbourhood y neighbourhood_group, la extensa lista de coordenadas y que no nos vamos a meter en el análisis de los puntos de interés turístico y la distancia de los airbnb con ellos vamos a prescindir de esta columna para seguir con el EDA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(['coordinates'], axis=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categoricas_analisis_1 = ['neighbourhood_group', 'room_type']\n",
    "\n",
    "# Crear una cuadrícula de gráficos\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(categoricas_analisis_1), figsize=(20, 6))\n",
    "\n",
    "# Iterar por cada variable categórica y graficar con porcentajes\n",
    "for i, col in enumerate(categoricas_analisis_1):\n",
    "    # Calcular el porcentaje\n",
    "    value_counts = df_new[col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    # Crear un gráfico de barras con porcentajes\n",
    "    axes[i].bar(value_counts.index, value_counts.values, color=sns.color_palette(\"viridis\", len(value_counts)))\n",
    "    \n",
    "    # Configuración del gráfico\n",
    "    axes[i].set_title(f'Distribución de {col} (%)')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Porcentaje (%)')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los gráficos de arriba podemos ver la cantidad de alojamientos que encontramos en cada agrupación de barrios y la cantidad de tipos de apartamentos / habitación en alquiler. \n",
    "Sobresaliendo Manhattan y Brooklyn en localización con más airbnb y los apartamentos completos seguidos de las habitaciones privadas como claras predominantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar los 20 barrios más representativos\n",
    "top_20_neighbourhoods = df_new['neighbourhood'].value_counts().head(20).index\n",
    "\n",
    "# Agrupar los barrios en una serie temporalmente para el gráfico\n",
    "neighbourhood_counts = df_new['neighbourhood'].apply(\n",
    "    lambda x: x if x in top_20_neighbourhoods else 'Otros'\n",
    ").value_counts()\n",
    "\n",
    "# Graficar la distribución de los barrios agrupados\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.bar(neighbourhood_counts.index, neighbourhood_counts.values, color=sns.color_palette('viridis', len(neighbourhood_counts)))\n",
    "plt.title('Distribución de Neighbourhood (Top 20 + Otros)')\n",
    "plt.xlabel('Neighbourhood')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta la cantidad de barrios que hay y que ya los tenemos agrupados en la columna 'neighbourhood_group' he decidido prescindir de esta columna para facilitar el modelo de machine learning aunque renuncie a una preción mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(['neighbourhood'], axis=1)\n",
    "df_new.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear una tabla cruzada para frecuencias absolutas\n",
    "heatmap_data = pd.crosstab(df_new['neighbourhood_group'], df_new['room_type'])\n",
    "\n",
    "# Calcular porcentajes\n",
    "heatmap_percent = heatmap_data.div(heatmap_data.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Crear un DataFrame con los porcentajes formateados para anotaciones\n",
    "annot_percent = heatmap_percent.round(1).astype(str) + '%'  # Formatear como texto con \"%\"\n",
    "\n",
    "# Crear una cuadrícula para mostrar ambos gráficos\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Gráfico 1: Heatmap de Frecuencia Absoluta\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\"d\", cmap=\"viridis\", cbar=True, ax=axes[0])\n",
    "axes[0].set_title('Frecuencia Absoluta de Room Type por Neighbourhood Group')\n",
    "axes[0].set_xlabel('Room Type')\n",
    "axes[0].set_ylabel('Neighbourhood Group')\n",
    "\n",
    "# Gráfico 2: Heatmap de Porcentajes\n",
    "sns.heatmap(\n",
    "    heatmap_percent, \n",
    "    annot=annot_percent,  # Usar el DataFrame de porcentajes formateados\n",
    "    fmt='',  # Para texto personalizado en las anotaciones\n",
    "    cmap=\"viridis\", \n",
    "    cbar=True, \n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Porcentaje de Room Type por Neighbourhood Group')\n",
    "axes[1].set_xlabel('Room Type')\n",
    "axes[1].set_ylabel('Neighbourhood Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los gráficos superiores que se representan tanto en porcentaje como en frecuencia podemos observar varias cosas relevantes para nuestro estudio: \n",
    "   \n",
    "- En Manhattan predominan los apartamentos/casas completos con un 60,9% por ciento del mercado. Además de ser la que más propiedades tiene. Podemos sugerir que estamos hablando un enfoque mayor al mercado de lujo.\n",
    "- En Brooklyn tenemos 50,4% de habitaciones privadas frente el 47,5% de establecimientos completos.   \n",
    "- En Queens, con un número ya sigificativamente menor de propiedades, podemos ver como también predominan con el 59,5% la habitación privada. Aún siendo una diferencia no muy grande aún.  \n",
    "- Por último, en el Bronx, con bastantes menos airbnbs sigue predominando la habitación privada. Será en el barrio donde más porcentaje de habitación compartida encontraremos, aún siendo pequeño (5,5%). Nos situamos en zonas con unos presupuestos más ajustados y menos turísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una tabla cruzada entre neighbourhood_group y room_type\n",
    "cross_tab = pd.crosstab(df_new['neighbourhood_group'], df_new['room_type'], normalize='index') * 100\n",
    "\n",
    "# Graficar la tabla cruzada como barras apiladas\n",
    "cross_tab.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "plt.title('Distribución de Room Type por Neighbourhood Group (%)')\n",
    "plt.xlabel('Neighbourhood Group')\n",
    "plt.ylabel('Porcentaje (%)')\n",
    "plt.legend(title='Room Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Análisis de las variables numéricas\n",
    "##### Hagamos un seguimiento de con qué variables nos hemos quedado por ahora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar las variables numéricas\n",
    "variables_numericas = df_new._get_numeric_data().columns\n",
    "print(\"Las variables numéricas son:\")\n",
    "for var in variables_numericas:\n",
    "    print(f\" - {var}\")\n",
    "\n",
    "# Identificar las variables categóricas\n",
    "variables_categoricas = set(df_new.columns) - set(variables_numericas)\n",
    "print(\"\\nLas variables categóricas son:\")\n",
    "for var in variables_categoricas:\n",
    "    print(f\" - {var}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumen estadístico y gráficos de frecuencia generales de las variables numéricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico general\n",
    "#df_new.describe()\n",
    "\n",
    "\n",
    "# Configuración de la cuadrícula: histogramas arriba y boxplots abajo\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 10), gridspec_kw={'height_ratios': [6, 1]})\n",
    "\n",
    "# Iterar sobre las variables numéricas\n",
    "for i, col in enumerate(variables_numericas):\n",
    "    row = 0  # Fila para el histograma\n",
    "    col_index = i % 3  # Columna en la cuadrícula\n",
    "    sns.histplot(data=df_new, x=col, kde=True, bins=30, ax=axes[row, col_index])  # Histograma\n",
    "    axes[row, col_index].set_title(f'Distribución de {col}')\n",
    "\n",
    "    row = 1  # Fila para el boxplot\n",
    "    sns.boxplot(data=df_new, x=col, ax=axes[row, col_index])  # Boxplot\n",
    "    axes[row, col_index].set_xlabel(col)\n",
    "\n",
    "# Ajustar diseño para evitar solapamientos\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar los gráficos\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precio: La mayoría de los precios están por debajo de aprox 200$, pero hay valores extremos muy altos.\n",
    "- Mínimo de noches: La mayor parte de las estancias son cortas pero con los valores extremos no podemos visualizarlo bien.\n",
    "- Número de reviews: La mayor parte tienen muy pocas reviews y unas pocas destacan. \n",
    "- Reseñas por mes: Muchas propiedades no tienen actividad, chequear las que tienen más para identificar patrones. Decido quedarme con el nº total de reviews y elimino esta columna.\n",
    "- Propiedades por anfitrión: La mayor parte tiene una sola propiedad, omitiremos esta columna. No creo que sea significativa.\n",
    "- Disponibilidad anual: Relacionar con precios.\n",
    "\n",
    "Hay muchos outliers a trabajar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(['calculated_host_listings_count', 'reviews_per_month'], axis=1)\n",
    "\n",
    "# Identificar las variables numéricas\n",
    "variables_numericas = df_new._get_numeric_data().columns\n",
    "print(\"Las variables numéricas son:\")\n",
    "for var in variables_numericas:\n",
    "    print(f\" - {var}\")\n",
    "\n",
    "# Identificar las variables categóricas\n",
    "variables_categoricas = set(df_new.columns) - set(variables_numericas)\n",
    "print(\"\\nLas variables categóricas son:\")\n",
    "for var in variables_categoricas:\n",
    "    print(f\" - {var}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis numérico-numérico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(combinations(variables_numericas, 2))  # Generar todas las combinaciones de pares\n",
    "\n",
    "# Determinar el tamaño dinámico de la cuadrícula\n",
    "rows = len(pairs) // 2 + len(pairs) % 2\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(15, rows * 3))\n",
    "\n",
    "for idx, (col1, col2) in enumerate(pairs):\n",
    "    row, col = divmod(idx, 2)  # Calcular posición de la cuadrícula\n",
    "    sns.regplot(ax=axes[row, col], data=df_new, x=col1, y=col2, scatter_kws={'alpha': 0.5})\n",
    "    axes[row, col].set_title(f'{col1} vs {col2}')\n",
    "    corr = df_new[[col1, col2]].corr().iloc[0, 1]\n",
    "    axes[row, col].annotate(f\"Corr: {corr:.2f}\", xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "                            fontsize=10, color='red', ha='left', va='top')\n",
    "\n",
    "# Eliminar espacios vacíos si no se usan todas las subplots\n",
    "if len(pairs) % 2 != 0:\n",
    "    fig.delaxes(axes[-1, -1])  # Eliminar el último eje si no hay suficientes pares\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusiones:\n",
    "\n",
    "- Precio y Noches Mínimas: Estos datos no tienen una relación clara, por lo que probablemente no sea necesario realizar más análisis. Nada significativa.\n",
    "- Precio y nº de reviews: Tiene una corelación muy floja, se ven acumuladas en precios más bajo. No significativo.\n",
    "- Precio y disponibilidad: Parece haber una relación muy débil donde las propiedades más disponibles tienen precios más altos. No significativo.\n",
    "- Mínimo de noches y reviews: Relación débil. Propiedades con menos noches mínimas pueden tener más reseñas. No significativo.\n",
    "- Mínimo de noches y disponibilidad: Parece que cuanto más aumentan las noches también la disponibilidad, aunque hay outliers. Relación debil.\n",
    "- Número de Reseñas y Disponibilidad: Existe una correlación leve que sugiere que la disponibilidad influye en el número de reseñas, pero no de manera muy significativa.\n",
    "\n",
    "Podemos ver que entre las variables numéricas las correlaciones son muy bajas. Tienen a tener poca relación entre si. Y podemos observar los outliers que trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Análisis multivariante\n",
    "#### Tras analizar las características una a una, es momento de analizarlas en relación con la predictoria y con ellas mismas para sacar conclusiones más claras acerca de sus relaciones y tomar decisiones sobre su procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de colores personalizados\n",
    "custom_palette = {\n",
    "    'Bronx': '#1f77b4',\n",
    "    'Brooklyn': '#ff7f0e',\n",
    "    'Manhattan': '#2ca02c',\n",
    "    'Queens': '#d62728',\n",
    "    'Staten Island': '#9467bd'\n",
    "}\n",
    "\n",
    "sns.catplot(\n",
    "    x='neighbourhood_group',\n",
    "    y='price',\n",
    "    data=df_new,\n",
    "    kind='bar',\n",
    "    hue='neighbourhood_group',\n",
    "    palette=custom_palette\n",
    ")\n",
    "\n",
    "plt.title(\"Precio Promedio por Neighbourhood Group\")\n",
    "plt.xlabel(\"Neighbourhood Group\")\n",
    "plt.ylabel(\"Precio Promedio\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos este gráfico antes de factorizar para tener en cuenta que si que hay diferencias significativas y están relacionados los precios con los barrios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Factorizar las variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar las variables categóricas con sus valores numéricos /\n",
    "for col in variables_categoricas:\n",
    "    df_new[col] = pd.factorize(df_new[col])[0]\n",
    "\n",
    "# Verificar el resultado\n",
    "df_new.head()\n",
    "\n",
    "#NO ESTA PERO ES MEJOR HACER COPIA DE LAS VARIABLES FACTORIZADAS Y MANTENER LA ORIGINAL\n",
    "\n",
    "# Crear copias factorizadas de las columnas categóricas mientras se preservan las originales\n",
    "# for col in variables_categoricas:\n",
    "#   df_new[f\"{col}_factorized\"] = pd.factorize(df_new[col])[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Categorías de 'neighbourhood_group' y sus valores numéricos:  \n",
    "  0 -> Brooklyn  \n",
    "  1 -> Manhattan  \n",
    "  2 -> Queens  \n",
    "  3 -> Staten Island  \n",
    "  4 -> Bronx  \n",
    "\n",
    "Categorías de 'room_type' y sus valores numéricos:  \n",
    "  0 -> Private room  \n",
    "  1 -> Entire home/apt  \n",
    "  2 -> Shared room  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar las variables numéricas\n",
    "variables_numericas = df_new._get_numeric_data().columns\n",
    "print(\"Las variables numéricas son:\")\n",
    "for var in variables_numericas:\n",
    "    print(f\" - {var}\")\n",
    "\n",
    "# Identificar las variables categóricas\n",
    "variables_categoricas = set(df_new.columns) - set(variables_numericas)\n",
    "print(\"\\nLas variables categóricas son:\")\n",
    "for var in variables_categoricas:\n",
    "    print(f\" - {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de correlación entre todas las variables numéricas\n",
    "correlation_matrix = df_new.corr()\n",
    "\n",
    "# Graficar el heatmap para visualizar las correlaciones\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", square=True)\n",
    "plt.title(\"Matriz de Correlación entre Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "sns.boxplot(data=df_new, x='neighbourhood_group', y='price', ax=axes[0])\n",
    "axes[0].set_title('Precio por Neighbourhood Group')\n",
    "\n",
    "sns.boxplot(data=df_new, x='room_type', y='price', ax=axes[1])\n",
    "axes[1].set_title('Precio por Room Type')\n",
    "\n",
    "sns.boxplot(data=df_new, x='room_type', y='minimum_nights', ax=axes[2])\n",
    "axes[2].set_title('Minimum Nights por Room Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = 'neighbourhood_group', y = 'price', data = df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 =df_new[df_new['price']<500]\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.violinplot(x = 'neighbourhood_group', y = 'price', data = df1, scale = 'count', linewidth = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaciones todos con todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relaciones todos con todos\n",
    "sns.pairplot(data = df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detectando y trabajando Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar outliers usando el rango intercuartílico (IQR) y agregar información detallada\n",
    "outliers_info = {}\n",
    "for col in variables_numericas:\n",
    "    Q1 = df_new[col].quantile(0.25)\n",
    "    Q3 = df_new[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Detectar los valores outliers\n",
    "    outliers = df_new[(df_new[col] < lower_bound) | (df_new[col] > upper_bound)]\n",
    "    num_outliers = len(outliers)\n",
    "    \n",
    "    # Guardar la información en el diccionario\n",
    "    outliers_info[col] = {\n",
    "        \"num_outliers\": num_outliers,\n",
    "        \"lower_bound\": lower_bound,\n",
    "        \"upper_bound\": upper_bound,\n",
    "        \"outlier_range_below\": outliers[col][outliers[col] < lower_bound].tolist(),\n",
    "        \"outlier_range_above\": outliers[col][outliers[col] > upper_bound].tolist()\n",
    "    }\n",
    "    \n",
    "    # Mostrar la información resumida para cada variable <3, qué bonito ha quedado\n",
    "    print(f\"Variable: {col}\")\n",
    "    print(f\"  Outliers detectados: {num_outliers}\")\n",
    "    print(f\"  Límite inferior: {lower_bound:.2f}\")\n",
    "    print(f\"  Límite superior: {upper_bound:.2f}\")\n",
    "    print()  # Espacio para separar variables\n",
    "\n",
    "#outliers_info  # Diccionario con toda la información de los outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de colores para los boxplots\n",
    "colors = sns.color_palette(\"pastel\", len(variables_numericas))\n",
    "\n",
    "# Configurar el layout de gráficos\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))  # Ajustar según el número de variables\n",
    "\n",
    "# Iterar sobre las variables numéricas y graficar boxplots\n",
    "for i, col in enumerate(variables_numericas):\n",
    "    row, col_index = divmod(i, 3)  # Calcular posición en la cuadrícula\n",
    "    sns.boxplot(data=df_new, y=col, ax=axes[row, col_index], color=colors[i])\n",
    "    axes[row, col_index].set_title(f'Boxplot de {col}')\n",
    "    axes[row, col_index].set_ylabel(col)\n",
    "\n",
    "# Ajustar el diseño para evitar solapamientos\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una copia del data set y uno lo trabajaremos con los outliers moldeados y otro con los datos originales.\n",
    "Guardaremos los outliers originales en un archivo .json para recurrir a ellos en caso necesario. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta para guardar el archivo JSON\n",
    "json_file_path = './data/outlier_limits.json'\n",
    "\n",
    "# Crear un diccionario para almacenar los límites de outliers\n",
    "outlier_limits = {}\n",
    "\n",
    "\n",
    "# Guardar los límites en un archivo JSON\n",
    "try:\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        json.dump(outlier_limits, f, indent=4)\n",
    "    print(f\"Límites de outliers guardados en: {json_file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo guardar el archivo JSON: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame sin outliers\n",
    "df_sin_outliers = df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar para eliminar los outliers según los límites calculados\n",
    "for col in outlier_limits.keys():\n",
    "    lower_bound = outlier_limits[col][\"lower_bound\"]\n",
    "    upper_bound = outlier_limits[col][\"upper_bound\"]\n",
    "    df_without_outliers = df_without_outliers[\n",
    "        (df_sin_outliers[col] >= lower_bound) & (df_sin_outliers[col] <= upper_bound)\n",
    "    ]\n",
    "\n",
    "df_original = df_new # cambio el nomntr del df para luego aclararme\n",
    "\n",
    "# Comparar tamaños\n",
    "print(\"Tamaño original:\", df_original.shape)\n",
    "print(\"Tamaño sin outliers:\", df_sin_outliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de datos \n",
    "\n",
    "Cuando entrenamos modelos de Machine Learning necesitamos:\n",
    "\n",
    "- Un conjunto de entrenamiento (train) para que el modelo aprenda los patrones de los datos\n",
    "- Un conjunto de prueba (test) para evaluar el rendimiento del modelo en datos nuevos\n",
    "  \n",
    "Por lo general dividimos los datos en 80% para entrenamiento y un 20% para prueba.\n",
    "\n",
    "#### 1. Identificar las variables predictorias y variable objetivo   \n",
    "  \n",
    "- Variable predictoria (x): Columnas que usará el modelo para predecir\n",
    "- Variable objetivo (y): La que queremos predecir (en este caso 'price')\n",
    "  \n",
    "#### 2. Usar train_test_split (división automática en conjuntos de prueba y datos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sin_outliers = df_sin_outliers.drop(columns=['price']) #[aquí pondríamos las variables a trabajar en este caso no porque son todas numericas y trabajamos todas menos la objetiva]  #variable predictoria donde moldearemos outliers\n",
    "x_original = df_original.drop(columns=['price']) # variable predictoria original\n",
    "y = df_original['price'] #variable objetiva / esta no se toca \n",
    "\n",
    "\n",
    "# División 1: Conjunto original con outliers\n",
    "x_train_con_outliers, x_test_con_outliers, y_train, y_test= train_test_split(x_original, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# División 2: Conjunto sin outliers\n",
    "x_train_sin_outliers, x_test_sin_outliers, y_train, y_test= train_test_split(x_sin_outliers, y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train_con_outliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implimentar Escalado y normalización\n",
    "\n",
    "#### ¿Por qué hacer escalado y normalización?\n",
    "  \n",
    "- 'price' podría estar en miles, mientras que number_of_reviews puede estar entre 0 y 600.\n",
    "- Esto puede llevar a que los modelos den más peso a las variables con valores más altos, afectando su rendimiento.\n",
    "  \n",
    "Para evitar esto, hacemos escalado o normalización. Esto asegura que todas las variables tengan un rango uniforme, ayudando al modelo a converger mejor y aprender más rápido.\n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NORMALIZACIÓN (Z-SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creamos escalador Z-Score \n",
    "scaler_norm = StandardScaler()\n",
    "\n",
    "# Ajustar el escalador a los datos de entrenamiento\n",
    "scaler_norm.fit(x_train_sin_outliers)\n",
    "\n",
    "#Transformar los datos de entrenamiento \n",
    "x_train_norm = scaler_norm.transform(x_train_sin_outliers)\n",
    "x_test_norm = scaler_norm.transform(x_test_sin_outliers)\n",
    "\n",
    "# Convertir resultado en DFs\n",
    "x_train_norm = pd.DataFrame(x_train_norm, index=x_train_sin_outliers.index, columns=x_train_sin_outliers.columns)\n",
    "x_test_norm = pd.DataFrame(x_test_norm, index=x_test_sin_outliers.index, columns=x_test_sin_outliers.columns)\n",
    "\n",
    "\n",
    "# Guardar el escalador\n",
    "dump(scaler_norm, open(\"scaler_norm.sav\", \"wb\"))\n",
    "\n",
    "\n",
    "# Mostrar ejemplos de los datos normalizados\n",
    "print(\"Datos normalizados - Entrenamiento:\")\n",
    "print(x_train_norm.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ESCALADO (Min-Max Scaling)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el escalador Min-Max\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# Ajustar el escalador a los datos de entrenamiento sin outliers\n",
    "scaler_minmax.fit(x_train_sin_outliers)\n",
    "\n",
    "# Ajustar a datos de entrenamiento\n",
    "x_train_scaled = scaler_minmax.transform(x_train_sin_outliers)\n",
    "x_test_scaled = scaler_minmax.transform(x_test_sin_outliers)\n",
    "\n",
    "# Convertir los resultados a DF\n",
    "x_train_scaled = pd.DataFrame(x_train_scaled, index=x_train_sin_outliers.index, columns=x_train_sin_outliers.columns)\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled, index=x_test_sin_outliers.index, columns=x_test_sin_outliers.columns)\n",
    "#guardar el escalador Min-Max\n",
    "dump(scaler_minmax, open(\"scaler_minmax.sav\", \"wb\"))\n",
    "\n",
    "# Muestra\n",
    "print(\"\\nDatos escalados Min-Max - Entrenamiento:\")\n",
    "print(x_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO DEL MODELO\n",
    "\n",
    "# REGRESIÓN LINEAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar resultados\n",
    "resultados = {}\n",
    "\n",
    "# 1. Modelo en datos originales (con outliers)\n",
    "lr_con = LinearRegression()\n",
    "lr_con.fit(x_train_con_outliers, y_train)\n",
    "y_pred_con = lr_con.predict(x_test_con_outliers)\n",
    "resultados[\"Con Outliers\"] = {\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_con)),\n",
    "    \"R²\": r2_score(y_test, y_pred_con),\n",
    "}\n",
    "\n",
    "# 2. Modelo en datos normalizados (Z-Score)\n",
    "lr_norm = LinearRegression()\n",
    "lr_norm.fit(x_train_norm, y_train)\n",
    "y_pred_norm = lr_norm.predict(x_test_norm)\n",
    "resultados[\"Normalizado (Z-Score)\"] = {\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_norm)),\n",
    "    \"R²\": r2_score(y_test, y_pred_norm),\n",
    "}\n",
    "\n",
    "# 3. Modelo en datos escalados (Min-Max)\n",
    "lr_scaled = LinearRegression()\n",
    "lr_scaled.fit(x_train_scaled, y_train)\n",
    "y_pred_scaled = lr_scaled.predict(x_test_scaled)\n",
    "resultados[\"Escalado (Min-Max)\"] = {\n",
    "    \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_scaled)),\n",
    "    \"R²\": r2_score(y_test, y_pred_scaled),\n",
    "}\n",
    "\n",
    "# Mostrar resultados\n",
    "for conjunto, metricas in resultados.items():\n",
    "    print(f\"\\nResultados para {conjunto}:\")\n",
    "    print(f\" - RMSE: {metricas['RMSE']:.2f}\")\n",
    "    print(f\" - R²: {metricas['R²']:.2f}\")\n",
    "    print(\"\\nJAJAJAJA QUE PENA DE RESULTADOS PARA TANTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Entrenar un modelo Random Forest\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(x_train_con_outliers, y_train)\n",
    "y_pred_rf = rf.predict(x_test_con_outliers)\n",
    "\n",
    "# Evaluar el modelo\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\nResultados para Random Forest:\")\n",
    "print(f\" - RMSE: {rmse_rf:.2f}\")\n",
    "print(f\" - R²: {r2_rf:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
